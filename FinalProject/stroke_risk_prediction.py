# -*- coding: utf-8 -*-
"""Stroke risk prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eCwS2bVzvfKSbChWlNKm9iPGkwOvacM_

## IMPORT
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.neighbors import LocalOutlierFactor as LOF
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

from google.colab import files

upload = files.upload()

# from google.colab import drive
# drive.mount('/content/drive')

# path = "/content/drive/MyDrive/DM/stroke_prediction_dataset.csv"

data = pd.read_csv('stroke_prediction_dataset.csv')
# data = pd.read_csv(path)
# data.head()

"""## DATA CLEANING"""

!pip install dataprep
from dataprep.eda import plot
plot(data, 'Diagnosis', 'Symptoms')

plot(data, 'Diagnosis', 'Cholesterol Levels')

plot(data, 'Diagnosis', 'Blood Pressure Levels')

plot(data, 'Diagnosis', 'Stress Levels')

plot(data, 'Diagnosis', 'Dietary Habits')

plot(data, 'Diagnosis', 'Family History of Stroke')

plot(data, 'Diagnosis', 'Stroke History')

plot(data, 'Diagnosis', 'Physical Activity')

plot(data, 'Diagnosis', 'Smoking Status')

plot(data, 'Diagnosis', 'Alcohol Intake')

plot(data, 'Diagnosis', 'Work Type')

plot(data, 'Diagnosis', 'Marital Status')

plot(data, 'Diagnosis', 'Heart Disease')

plot(data, 'Diagnosis', 'Hypertension')

plot(data, 'Diagnosis', 'Residence Type')

plot(data, 'Diagnosis', 'Age')

plot(data, 'Diagnosis', 'Gender')

plot(data, 'Diagnosis', 'Average Glucose Level')

plot(data, 'Diagnosis', 'Body Mass Index (BMI)')

# Blood Pressure and Cholesterol Levels to a good format
df_corr = pd.DataFrame(data)

df_corr['Blood Pressure Levels'] = data['Blood Pressure Levels'].str.split('/', expand=True).astype(float).assign(Blood_Pressure_Levels=lambda x: round(x[0] / x[1], 2))['Blood_Pressure_Levels']
df_corr['HDL'] = data['Cholesterol Levels'].str.extract('(HDL: \d+)')[0].str.replace('HDL: ', '').astype(int)
df_corr['LDL'] = data['Cholesterol Levels'].str.extract('(LDL: \d+)')[0].str.replace('LDL: ', '').astype(int)

#Clean the dataset

df_corr = data.drop(columns=['Patient Name', 'Patient ID','Cholesterol Levels', 'Symptoms'], axis = 1)

df_corr['Gender'] = pd.Categorical(data['Gender'], ordered=True)
df_corr['Marital Status'] = pd.Categorical(data['Marital Status'], ordered=True)
df_corr['Work Type'] = pd.Categorical(data['Work Type'], ordered=True)
df_corr['Residence Type'] = pd.Categorical(data['Residence Type'], ordered=True)
df_corr['Smoking Status'] = pd.Categorical(data['Smoking Status'], ordered=True)
df_corr['Alcohol Intake'] = pd.Categorical(data['Alcohol Intake'], ordered=True)
df_corr['Physical Activity'] = pd.Categorical(data['Physical Activity'], ordered=True)
df_corr['Family History of Stroke'] = pd.Categorical(data['Family History of Stroke'], ordered=True)
df_corr['Dietary Habits'] = pd.Categorical(data['Dietary Habits'], ordered=True)
df_corr['Diagnosis'] = pd.Categorical(data['Diagnosis'], ordered=True)

print(df_corr['Gender'].cat.categories)
print(df_corr['Marital Status'].cat.categories)
print(df_corr['Work Type'].cat.categories)
print(df_corr['Residence Type'].cat.categories)
print(df_corr['Smoking Status'].cat.categories)
print(df_corr['Alcohol Intake'].cat.categories)
print(df_corr['Physical Activity'].cat.categories)
print(df_corr['Family History of Stroke'].cat.categories)
print(df_corr['Dietary Habits'].cat.categories)
print(df_corr['Diagnosis'].cat.categories)

df_corr['Gender'] = df_corr['Gender'].cat.codes
df_corr['Marital Status'] = df_corr['Marital Status'].cat.codes
df_corr['Work Type'] = df_corr['Work Type'].cat.codes
df_corr['Residence Type'] = df_corr['Residence Type'].cat.codes
df_corr['Smoking Status'] = df_corr['Smoking Status'].cat.codes
df_corr['Alcohol Intake'] = df_corr['Alcohol Intake'].cat.codes
df_corr['Physical Activity'] = df_corr['Physical Activity'].cat.codes
df_corr['Family History of Stroke'] = df_corr['Family History of Stroke'].cat.codes
df_corr['Dietary Habits'] = df_corr['Dietary Habits'].cat.codes
df_corr['Diagnosis'] = df_corr['Diagnosis'].cat.codes

df_corr.head()

df_corr.to_csv('Clean dataset.csv', index=False)
# Export data frame as csv
from google.colab import files
files.download('Clean dataset.csv')

"""## CHECK THE CORRELATION"""

corrM = df_corr.corr()
drawCorr = pd.DataFrame(np.abs(corrM)["Diagnosis"])
drawCorr = drawCorr.sort_values(by=['Diagnosis'], ascending=False)
print(drawCorr)

fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(data = np.abs(corrM), annot = True, cmap='coolwarm', fmt = ".2f", vmax = 0.03)

correlation_with_diagnosis = corrM['Diagnosis']

# Creation of a DataFrame with the correlation column specific to 'Diagnosis'
correlation_df = pd.DataFrame(correlation_with_diagnosis)

# Sort the DataFrame by correlation values in descending order
correlation_df_sorted = correlation_df.sort_values(by='Diagnosis', ascending=False)

# Creation of a heatmap with Seaborn for the 'Diagnosis' column
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_df_sorted, annot=True, cmap='coolwarm', fmt=".5f", linewidths=.5, vmax=0.03)
plt.show()

"""## CLUSTER
#### according to the cleaning outcome, we choose the following variables:
#### "Average Glucose Level, HDL, Stress Level, Hypertension, BMI"
####to analysis.

###Hierachical Cluster
"""

# select the column we need
selected_columns = ["Average Glucose Level", "HDL","LDL", "Stress Levels", "Hypertension", "Body Mass Index (BMI)"]
used_df = df_corr[selected_columns]

used_df.head()

"""####Ward"""

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))
plt.title("Stroke Dendograms with ward")
dend_ward = shc.dendrogram(shc.linkage(used_df, method='ward'))

# 繪製散佈圖查看分群情況
# affinity='euclidean', linkage='ward'
from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
cluster.fit_predict(used_df)
cluster1_labels = cluster.fit_predict(used_df)

plt.figure(figsize=(10, 7))
plt.xlabel("Average Glucose Level")
plt.ylabel("LDL")
plt.scatter(used_df["Average Glucose Level"], used_df["LDL"], c=cluster1_labels, cmap='rainbow')

cluster_centers = []

for label in set(cluster1_labels):
    cluster_center = used_df.loc[cluster1_labels == label].mean()
    cluster_centers.append(cluster_center)

cluster_centers = np.array(cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(cluster_centers):
    plt.scatter(center[0], center[2], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

plt.legend()
plt.show()

# 繪製散佈圖查看分群情況
plt.figure(figsize=(10, 7))
plt.scatter(used_df["Average Glucose Level"], used_df["HDL"], c=cluster1_labels, cmap='rainbow')

# 添加坐标轴标签
plt.xlabel("Average Glucose Level")
plt.ylabel("HDL")

# 獲取每個群組的中心點
ward1_cluster_centers = []

for label in set(cluster1_labels):
    cluster_centers = used_df.loc[cluster1_labels == label].mean()
    ward1_cluster_centers.append(cluster_centers)

ward1_cluster_centers = np.array(ward1_cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(ward1_cluster_centers):
    plt.scatter(center[0], center[1], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

plt.legend()
plt.show()

# 繪製3D散佈圖查看分群情況
fig = plt.figure(figsize=(18, 10))
ax = fig.add_subplot(111, projection='3d')

# 繪製散點圖
ax.scatter(used_df["HDL"], used_df["LDL"], used_df["Average Glucose Level"], c=cluster1_labels, cmap='rainbow')

# 添加坐标轴标签
ax.set_xlabel("HDL")
ax.set_ylabel("LDL")
ax.set_zlabel("Average Glucose Level")

# 獲取每個群組的中心點
ward1_cluster_centers = []

for label in set(cluster1_labels):
    cluster_centers = used_df.loc[cluster1_labels == label].mean()
    ward1_cluster_centers.append(cluster_centers)

ward1_cluster_centers = np.array(ward1_cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(ward1_cluster_centers):
    ax.scatter(center[1], center[2], center[0], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

plt.legend()
plt.show()

from sklearn.metrics import pairwise_distances_argmin_min

# 使用群中心的前三個特徵
ward1_cluster_centers_3d = ward1_cluster_centers[:, :3]

# 計算每個樣本點到其所屬群中心的距離
distances = pairwise_distances_argmin_min(used_df[['HDL', 'Stress Levels', 'Average Glucose Level']], ward1_cluster_centers_3d)[1]

# 將距離平方，再取平均值
ward1_average_squared_distance = np.mean(np.square(distances))

print(f"Average of each data to it's cluster center: {ward1_average_squared_distance}")

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, ward1_cluster_centers)

# 計算每個群的平均距離
average_distances_per_cluster = []
for label in set(cluster1_labels):
    distances_to_current_center = distances_to_centers[cluster1_labels == label, label]
    average_distance_to_current_center = np.mean(distances_to_current_center)
    average_distances_per_cluster.append(average_distance_to_current_center)

total_avg = []

# 打印每個群的平均距離
for i, avg_distance in enumerate(average_distances_per_cluster):
    print(f'Average Distance for Cluster {i+1}: {avg_distance}')
    total_avg.append(avg_distance)

total_avg_distance = np.mean(total_avg)
print("AVG of total distance:" + str(total_avg_distance))

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, ward1_cluster_centers)

# 計算群間平均距離
inter_cluster_distances = []
unique_labels = set(cluster1_labels)
for i, label_i in enumerate(unique_labels):
    for j, label_j in enumerate(unique_labels):
        if i < j:  # 避免計算兩次同樣的群組組合
            distances_between_clusters = euclidean_distances(ward1_cluster_centers[label_i].reshape(1, -1),
                                                              ward1_cluster_centers[label_j].reshape(1, -1))
            inter_cluster_distances.extend(distances_between_clusters.flatten())

# 計算群和群間平均距離
average_inter_cluster_distance = np.mean(inter_cluster_distances)

print(f"Average Inter-Cluster Distance: {average_inter_cluster_distance}")

from collections import Counter

# 使用 Counter 計算每個標籤的數量
cluster1_sizes = Counter(cluster1_labels)

# 輸出每一群的數量
for label, size in cluster1_sizes.items():
    print(f"Cluster {label + 1}: {size} samples")

# Add Cluster column to DataFrame
used_df['Cluster'] = cluster1_labels
# Group by Cluster column and calculate average
ward1_cluster_means = used_df.groupby('Cluster').mean()
# Print out the average of each variable in each cluster
ward1_cluster_means

"""####Complete linkage"""

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))
plt.title("Stroke Dendograms with complete")
dend_complete = shc.dendrogram(shc.linkage(used_df, method='complete'))

# 繪製散佈圖查看分群情況
# affinity='euclidean', linkage='complete'
from sklearn.cluster import AgglomerativeClustering

cluster_complete = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')
cluster2_labels = cluster_complete.fit_predict(used_df)

plt.figure(figsize=(10, 7))
plt.xlabel("Average Glucose Level")
plt.ylabel("LDL")
plt.scatter(used_df["Average Glucose Level"], used_df["LDL"], c=cluster2_labels, cmap='rainbow')


cluster_centers = []

for label in set(cluster2_labels):
    cluster_center = used_df.loc[cluster2_labels == label].mean()
    cluster_centers.append(cluster_center)

cluster_centers = np.array(cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(cluster_centers):
    plt.scatter(center[0], center[2], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

plt.legend()
plt.show()

# 繪製3D散佈圖查看分群情況
fig = plt.figure(figsize=(18, 10))
ax = fig.add_subplot(111, projection='3d')

# 繪製散點圖
scatter = ax.scatter(used_df["Average Glucose Level"], used_df["LDL"], used_df["HDL"], c=cluster2_labels, cmap='rainbow')

# 添加坐标轴标签
ax.set_xlabel("Average Glucose Level")
ax.set_ylabel("LDL")
ax.set_zlabel("HDL")

# 添加色條
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

# 獲取每個群組的中心點
complete1_cluster_centers = []

for label in set(cluster2_labels):
    cluster_centers = used_df.loc[cluster2_labels == label].mean()
    complete1_cluster_centers.append(cluster_centers)

complete1_cluster_centers = np.array(complete1_cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(complete1_cluster_centers):
    ax.scatter(center[0], center[2], center[1], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

    # 添加文字標籤
    ax.text(center[0], center[2], center[1], f'Cluster {i+1} Center', fontsize=12, color='black', ha='left')


plt.show()

from sklearn.metrics import pairwise_distances_argmin_min

# # 轉換成 NumPy 數組
# complete6_cluster_centers_np = np.array(complete6_cluster_centers)

# 使用群中心的前三個特徵
complete1_cluster_centers_3d = complete1_cluster_centers[:, :3]

# 計算每個樣本點到其所屬群中心的距離
distances = pairwise_distances_argmin_min(used_df[['HDL', 'Stress Levels', 'Average Glucose Level']], complete1_cluster_centers_3d)[1]

# 將距離平方，再取平均值
complete1_average_squared_distance = np.mean(np.square(distances))

print(f"每個群的平均分散程度: {complete1_average_squared_distance}")

from collections import Counter

# 使用 Counter 計算每個標籤的數量
cluster2_sizes = Counter(cluster2_labels)

# 輸出每一群的數量
for label, size in cluster2_sizes.items():
    print(f"Cluster {label + 1}: {size} samples")

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, complete1_cluster_centers)

# 計算每個群的平均距離
average_distances_per_cluster = []
for label in set(cluster2_labels):
    distances_to_current_center = distances_to_centers[cluster2_labels == label, label]
    average_distance_to_current_center = np.mean(distances_to_current_center)
    average_distances_per_cluster.append(average_distance_to_current_center)

total_avg = []

# 打印每個群的平均距離
for i, avg_distance in enumerate(average_distances_per_cluster):
    print(f'Average Distance for Cluster {i+1}: {avg_distance}')
    total_avg.append(avg_distance)

total_avg_distance = np.mean(total_avg)
print("AVG of total distance:" + str(total_avg_distance))

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, complete1_cluster_centers)

# 計算群間平均距離
inter_cluster_distances = []
unique_labels = set(cluster2_labels)
for i, label_i in enumerate(unique_labels):
    for j, label_j in enumerate(unique_labels):
        if i < j:  # 避免計算兩次同樣的群組組合
            distances_between_clusters = euclidean_distances(complete1_cluster_centers[label_i].reshape(1, -1),
                                                              complete1_cluster_centers[label_j].reshape(1, -1))
            inter_cluster_distances.extend(distances_between_clusters.flatten())

# 計算群和群間平均距離
average_inter_cluster_distance = np.mean(inter_cluster_distances)

print(f"Average Inter-Cluster Distance: {average_inter_cluster_distance}")

# 印出群中心的值
print("Cluster Centers:")
print(complete1_cluster_centers)

# 檢查是否有重複的中心點
print("Are there duplicate cluster centers?")
print(len(complete1_cluster_centers) != len(set(map(tuple, complete1_cluster_centers))))

import pandas as pd

# Add Cluster column to DataFrame
used_df['Cluster'] = cluster2_labels

# Group by Cluster column and calculate average
complete1_cluster_means = used_df.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
complete1_cluster_means

"""####Centroid"""

plt.figure(figsize=(10, 7))
plt.title("Stroke Dendograms with centroid")
dend_centroid = shc.dendrogram(shc.linkage(used_df, method='centroid'))

cluster_centroid = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='centroid')
cluster3_labels = cluster_complete.fit_predict(used_df)

plt.figure(figsize=(10, 7))
plt.xlabel("Average Glucose Level")
plt.ylabel("LDL")
plt.scatter(used_df["Average Glucose Level"], used_df["LDL"], c=cluster3_labels, cmap='rainbow')

cluster_centers = []

for label in set(cluster3_labels):
    cluster_center = used_df.loc[cluster3_labels == label].mean()
    cluster_centers.append(cluster_center)

cluster_centers = np.array(cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(cluster_centers):
    plt.scatter(center[0], center[2], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

plt.legend()
plt.show()

from sklearn.cluster import AgglomerativeClustering
from mpl_toolkits.mplot3d import Axes3D

# Use AgglomerativeClustering for clustering with centroid linkage
cluster_centroid = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='centroid')

cluster3_labels = cluster.fit_predict(used_df)

# Draw a 3D scatter plot to show clustering
fig = plt.figure(figsize=(18, 10))
ax = fig.add_subplot(111, projection='3d')

# Draw scatter plot
scatter = ax.scatter(used_df["Average Glucose Level"], used_df["LDL"], used_df["HDL"], c=cluster3_labels, cmap='rainbow')

# Add labels to the axes
ax.set_xlabel("Average Glucose Level")
ax.set_ylabel("LDL")
ax.set_zlabel("HDL")

# Add colorbar
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

# Get the center of each cluster
centroid1_cluster_centers = []

for label in set(cluster3_labels):
    cluster_center = used_df.loc[cluster3_labels == label].mean()
    centroid1_cluster_centers.append(cluster_center)

centroid1_cluster_centers = np.array(centroid1_cluster_centers)

# Draw the center point of each cluster and add a label
for i, center in enumerate(centroid1_cluster_centers):
    ax.scatter(center[0], center[2], center[1], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

    # Add text labels
    ax.text(center[0], center[2], center[1], f'Cluster {i+1} Center', fontsize=12, color='black', ha='left')

plt.show()

from sklearn.metrics import pairwise_distances_argmin_min

# # 轉換成 NumPy 數組
# centroid3_cluster_centers_np = np.array(centroid3_cluster_centers)

# 使用群中心的前三個特徵
centroid1_cluster_centers_3d = centroid1_cluster_centers[:, :3]

# 計算每個樣本點到其所屬群中心的距離
distances = pairwise_distances_argmin_min(used_df[['HDL', 'Stress Levels', 'Average Glucose Level']], centroid1_cluster_centers_3d)[1]

# 將距離平方，再取平均值
centroid1_average_squared_distance = np.mean(np.square(distances))

print(f"每個群的平均分散程度: {centroid1_average_squared_distance}")

from collections import Counter

# 使用 Counter 計算每個標籤的數量
cluster3_sizes = Counter(cluster3_labels)

# 輸出每一群的數量
for label, size in cluster3_sizes.items():
    print(f"Cluster {label + 1}: {size} samples")

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, centroid1_cluster_centers)

# 計算每個群的平均距離
average_distances_per_cluster = []
for label in set(cluster3_labels):
    distances_to_current_center = distances_to_centers[cluster3_labels == label, label]
    average_distance_to_current_center = np.mean(distances_to_current_center)
    average_distances_per_cluster.append(average_distance_to_current_center)

total_avg = []

# 打印每個群的平均距離
for i, avg_distance in enumerate(average_distances_per_cluster):
    print(f'Average Distance for Cluster {i+1}: {avg_distance}')
    total_avg.append(avg_distance)

total_avg_distance = np.mean(total_avg)
print("AVG of total distance:" + str(total_avg_distance))

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, centroid1_cluster_centers)

# 計算群間平均距離
inter_cluster_distances = []
unique_labels = set(cluster3_labels)
for i, label_i in enumerate(unique_labels):
    for j, label_j in enumerate(unique_labels):
        if i < j:  # 避免計算兩次同樣的群組組合
            distances_between_clusters = euclidean_distances(centroid1_cluster_centers[label_i].reshape(1, -1),
                                                              centroid1_cluster_centers[label_j].reshape(1, -1))
            inter_cluster_distances.extend(distances_between_clusters.flatten())

# 計算群和群間平均距離
average_inter_cluster_distance = np.mean(inter_cluster_distances)

print(f"Average Inter-Cluster Distance: {average_inter_cluster_distance}")



import pandas as pd

# Add Cluster column to DataFrame
used_df['Cluster'] = cluster3_labels

# Group by Cluster column and calculate average
centroid1_cluster_means = used_df.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
centroid1_cluster_means

"""####Try to use normalized data to cluster"""

scaler = StandardScaler()
used_df_normalized = scaler.fit_transform(used_df)

#Average Glucose Level, HDL, Stress Levels, Hypertension, Body Mass Index (BMI)
# after Standardlized

from mpl_toolkits.mplot3d import Axes3D

# 繪製3D散佈圖查看分群情況
fig = plt.figure(figsize=(18, 10))
ax = fig.add_subplot(111, projection='3d')

cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')

used_df_normalized = pd.DataFrame(used_df_normalized, columns=used_df.columns)

cluster_labels_int = cluster.fit_predict(used_df_normalized)


# 繪製散點圖
scatter = ax.scatter(used_df_normalized["HDL"], used_df_normalized["Stress Levels"], used_df_normalized["Average Glucose Level"], c=cluster_labels_int, cmap='rainbow')

# 添加坐标轴标签
ax.set_xlabel("HDL")
ax.set_ylabel("Stress Levels")
ax.set_zlabel("Average Glucose Level")

# 添加色條
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

# 獲取每個群組的中心點
cluster_centers = []

for label in set(cluster_labels_int):
    cluster_center = used_df_normalized.loc[cluster_labels_int == label].mean().values
    cluster_centers.append(cluster_center)

cluster_centers = np.array(cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(cluster_centers):
    ax.scatter(center[0], center[1], center[2], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

    # 添加文字標籤
    ax.text(center[0], center[1], center[2], f'Cluster {i+1} Center', fontsize=12, color='black', ha='left')

plt.show()

from sklearn.cluster import AgglomerativeClustering

# 使用AgglomerativeClustering進行分群
cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
cluster_labels00 = cluster.fit_predict(used_df_normalized)

# 繪製散佈圖查看分群情況
plt.figure(figsize=(10, 7))
plt.scatter(used_df_normalized["Average Glucose Level"], used_df_normalized["Stress Levels"], c=cluster_labels00, cmap='rainbow')

# 添加坐标轴标签
plt.xlabel("Average Glucose Level")
plt.ylabel("Stress Levels")

plt.show()

#Average Glucose Level, HDL, Stress Levels, Hypertension, Body Mass Index (BMI)
# after Standardlized

from mpl_toolkits.mplot3d import Axes3D

# 繪製3D散佈圖查看分群情況
fig = plt.figure(figsize=(18, 10))
ax = fig.add_subplot(111, projection='3d')

cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')

used_df_normalized = pd.DataFrame(used_df_normalized, columns=used_df.columns)

cluster_labels_int = cluster.fit_predict(used_df_normalized)


# 繪製散點圖
scatter = ax.scatter(used_df_normalized["HDL"], used_df_normalized["Stress Levels"], used_df_normalized["Average Glucose Level"], c=cluster_labels_int, cmap='rainbow')

# 添加坐标轴标签
ax.set_xlabel("HDL")
ax.set_ylabel("Stress Levels")
ax.set_zlabel("Average Glucose Level")

# 添加色條
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

# 獲取每個群組的中心點
cluster_centers = []

for label in set(cluster_labels_int):
    cluster_center = used_df_normalized.loc[cluster_labels_int == label].mean().values
    cluster_centers.append(cluster_center)

cluster_centers = np.array(cluster_centers)

# 在每個群組中心點上標記文字
for i, center in enumerate(cluster_centers):
    ax.scatter(center[0], center[1], center[2], s=200, marker='X', c='black', label=f'Cluster {i+1} Center')

    # 添加文字標籤
    ax.text(center[0], center[1], center[2], f'Cluster {i+1} Center', fontsize=12, color='black', ha='left')

plt.show()

"""捨棄標準化，直接分群，ward:4 group, complete:5 or 6 group, but still need to confirm which is better

###Non-Hierachical Cluster

####Kmeans
"""

# For Clustering
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer

"""Check the Elbow"""

# 未正規劃的elbow
distortions = []
K = range(1,10)
for k in K:
    kmean = KMeans(n_clusters=k,random_state=7)
    kmean.fit(used_df)
    distortions.append(kmean.inertia_)

plt.figure(figsize=(20,5))
plt.plot(K, distortions, '-',color='g')
plt.xlabel('k values')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

# # 正規劃後的elbow

# distortions = []
# K = range(1,10)
# for k in K:
#     kmean = KMeans(n_clusters=k,random_state=7)
#     kmean.fit(used_df_normalized)
#     distortions.append(kmean.inertia_)

# plt.figure(figsize=(20,5))
# plt.plot(K, distortions, '-',color='g')
# plt.xlabel('k values')
# plt.ylabel('Distortion')
# plt.title('The Elbow Method showing the optimal k')
# plt.show()

K = range(2,10)
silhouette = []
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(used_df)
    preds = kmeanModel.predict(used_df)
    silhouette.append(silhouette_score(used_df, preds))

plt.figure(figsize=(20,5))
plt.plot(K, silhouette, '-',color='g')
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score of each k values')
plt.show()

"""k=2"""

used_df = used_df.drop('Cluster',axis=1)

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns; sns.set()
from sklearn.cluster import KMeans

# 創建 KMeans 模型，設定群數為2
kmeans2 = KMeans(n_clusters=2, random_state=42)
# 對標準化後的資料進行分群
kmeans2_labels = kmeans2.fit_predict(used_df)

# 將分群的結果添加到原始資料中
used_df_copy = used_df.copy()
used_df_copy['Cluster'] = kmeans2_labels

# 繪製3D散佈圖
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# 繪製樣本散點圖
ax.scatter(used_df_copy["HDL"], used_df_copy["Average Glucose Level"], used_df_copy["LDL"], c=kmeans2_labels, cmap='rainbow', s=20)

# 繪製群中心
centers = kmeans2.cluster_centers_
ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='blue', s=100, alpha=0.9, marker='X')

# 設置坐標軸標籤
ax.set_xlabel("HDL")
ax.set_ylabel("Average Glucose Level")
ax.set_zlabel("LDL")

plt.show()

used_df_with_cluster2 = np.column_stack((used_df, kmeans2_labels))
used_df_with_cluster2
column_names = ['Average Glucose Level', 'HDL','LDL', 'Stress Levels', 'Hypertension', 'Body Mass Index (BMI)', 'Cluster']

# Change NumPy ro DataFrame
used_df_with_cluster2 = pd.DataFrame(used_df_with_cluster2, columns=column_names)

# # Save DataFrame in Excel
# used_df_with_cluster2.to_excel('used_df_with_cluster2.xlsx', index=False)
# # Export data frame as xlsx
# from google.colab import files
# files.download('used_df_with_cluster2.xlsx')

import pandas as pd

# Add Cluster column to DataFrame
used_df_with_cluster2['Cluster'] = kmeans2_labels

# Group by Cluster column and calculate average
kmeans1_cluster_means = used_df_with_cluster2.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
print(kmeans1_cluster_means)

# # Create DataFrame from original data and cluster labels
# used_df_with_cluster2 = pd.DataFrame({'Average Glucose Level': used_df_with_cluster2[:, 0],
#                                   'HDL': used_df_with_cluster2[:, 1],
#                                   'LDL': used_df_with_cluster2[:, 2],
#                                   'Stress Levels': used_df_with_cluster2[:, 3],
#                                   'Hypertension': used_df_with_cluster2[:, 4],
#                                   'Body Mass Index (BMI)': used_df_with_cluster2[:, 5],
#                                   'Cluster': used_df_with_cluster2[:, 6]})

# # Group by cluster label and calculate average
# kmeans_cluster2_means = used_df_with_cluster2.groupby('Cluster').mean()

# # Print out the average of each variable in each cluster
# print(kmeans_cluster2_means)

"""k=3"""

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns; sns.set()
from sklearn.cluster import KMeans

# 創建 KMeans 模型，設定群數為3
kmeans3 = KMeans(n_clusters=3, random_state=42)

# 對標準化後的資料進行分群
kmeans3_labels = kmeans3.fit_predict(used_df)

# 將分群的結果添加到原始資料中
used_df_copy = used_df.copy()
used_df_copy['Cluster'] = kmeans3_labels

# 繪製3D散佈圖
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# 繪製樣本散點圖
ax.scatter(used_df_copy["HDL"], used_df_copy["Average Glucose Level"], used_df_copy["LDL"], c=kmeans3_labels, cmap='rainbow', s=20)

# 繪製群中心
centers = kmeans3.cluster_centers_
ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], c='blue', s=100, alpha=0.9, marker='X')

# 設置坐標軸標籤
ax.set_xlabel("HDL")
ax.set_ylabel("Average Glucose Level")
ax.set_zlabel("LDL")

plt.show()

used_df_with_cluster3 = np.column_stack((used_df, kmeans3_labels))
used_df_with_cluster3
column_names = ['Average Glucose Level', 'HDL','LDL', 'Stress Levels', 'Hypertension', 'Body Mass Index (BMI)', 'Cluster']

# Change NumPy ro DataFrame
used_df_with_cluster3 = pd.DataFrame(used_df_with_cluster3, columns=column_names)

# # # Save DataFrame in Excel
# # used_df_with_cluster3.to_excel('used_df_with_cluster3.xlsx', index=False)
# # # Export data frame as xlsx
# # from google.colab import files
# # files.download('used_df_with_cluster3.xlsx')

import pandas as pd

# Add Cluster column to DataFrame
used_df_with_cluster3['Cluster'] = kmeans3_labels

# Group by Cluster column and calculate average
kmeans2_cluster_means = used_df_with_cluster3.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
print(kmeans2_cluster_means)

# # Create DataFrame from original data and cluster labels
# used_df_with_cluster3 = pd.DataFrame({'Average Glucose Level': used_df_with_cluster3[:, 0],
#                                   'HDL': used_df_with_cluster3[:, 1],
#                                   'LDL': used_df_with_cluster3[:, 2],
#                                   'Stress Levels': used_df_with_cluster3[:, 3],
#                                   'Hypertension': used_df_with_cluster3[:, 4],
#                                   'Body Mass Index (BMI)': used_df_with_cluster3[:, 5],
#                                   'Cluster': used_df_with_cluster3[:, 6]})

# # Group by cluster label and calculate average
# kmeans_cluster3_means = used_df_with_cluster3.groupby('Cluster').mean()

# # Print out the average of each variable in each cluster
# print(kmeans_cluster3_means)

"""k=4"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# 創建 KMeans 模型，設定群數為4
kmeans4_2d = KMeans(n_clusters=4, random_state=42)

# 對標準化後的資料進行分群
kmeans4_labels = kmeans4_2d.fit_predict(used_df)

# 將分群的結果添加到原始資料中
used_df_copy = used_df.copy()
used_df_copy['Cluster'] = kmeans4_labels

# 繪製2D散佈圖
plt.figure(figsize=(12, 8))

# 繪製樣本散點圖
plt.scatter(used_df_copy["Average Glucose Level"], used_df_copy["LDL"], c=kmeans4_labels, cmap='rainbow', s=20)

# 繪製群中心
centers = kmeans4_2d.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 2], c='blue', s=100, alpha=0.9, marker='X')

# 設置坐標軸標籤
plt.xlabel("Average Glucose Level")
plt.ylabel("LDL")

plt.show()

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns; sns.set()
from sklearn.cluster import KMeans

# 創建 KMeans 模型，設定群數為3
kmeans4 = KMeans(n_clusters=4, random_state=42)

# 對標準化後的資料進行分群
kmeans4_labels = kmeans4.fit_predict(used_df)

# 將分群的結果添加到原始資料中
used_df_copy = used_df.copy()
used_df_copy['Cluster'] = kmeans4_labels

# 繪製3D散佈圖
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# 繪製樣本散點圖
ax.scatter(used_df_copy["HDL"], used_df_copy["Average Glucose Level"], used_df_copy["LDL"], c=kmeans4_labels, cmap='rainbow', s=20)

# 繪製群中心
centers = kmeans4.cluster_centers_
ax.scatter(centers[:, 1], centers[:, 0], centers[:, 2], c='blue', s=100, alpha=0.9, marker='X')

# 設置坐標軸標籤
ax.set_xlabel("HDL")
ax.set_ylabel("Average Glucose Level")
ax.set_zlabel("LDL")

plt.show()

from collections import Counter

# 使用 Counter 計算每個標籤的數量
kmeans4_sizes = Counter(kmeans4_labels)

# 輸出每一群的數量
for label, size in kmeans4_sizes.items():
    print(f"Cluster {label + 1}: {size} samples")

# Add Cluster column to DataFrame
used_df['Cluster'] = kmeans4_labels
# Group by Cluster column and calculate average
kmeans4_cluster_means = used_df.groupby('Cluster').mean()
# Print out the average of each variable in each cluster
kmeans4_cluster_means

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, centers)

# 計算每個群的平均距離
average_distances_per_cluster = []
for label in set(kmeans4_labels):
    distances_to_current_center = distances_to_centers[kmeans4_labels == label, label]
    average_distance_to_current_center = np.mean(distances_to_current_center)
    average_distances_per_cluster.append(average_distance_to_current_center)

total_avg = []

# 打印每個群的平均距離
for i, avg_distance in enumerate(average_distances_per_cluster):
    print(f'Average Distance for Cluster {i+1}: {avg_distance}')
    total_avg.append(avg_distance)

total_avg_distance = np.mean(total_avg)
print("AVG of total distance:" + str(total_avg_distance))

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(used_df, centers)

# 計算群間平均距離
inter_cluster_distances = []
unique_labels = set(kmeans4_labels)
for i, label_i in enumerate(unique_labels):
    for j, label_j in enumerate(unique_labels):
        if i < j:  # 避免計算兩次同樣的群組組合
            distances_between_clusters = euclidean_distances(centers[label_i].reshape(1, -1), centers[label_j].reshape(1, -1))
            inter_cluster_distances.extend(distances_between_clusters.flatten())

# 計算群和群間平均距離
average_inter_cluster_distance = np.mean(inter_cluster_distances)

print(f"Average Inter-Cluster Distance: {average_inter_cluster_distance}")

selected_columns = ["Average Glucose Level", "HDL","LDL", "Stress Levels", "Hypertension", "Body Mass Index (BMI)"]
used_df_with_cluster4 = used_df_with_cluster3[selected_columns]
used_df_with_cluster4

import pandas as pd

used_df_with_cluster4['Cluster'] = kmeans4_labels

# Change NumPy ro DataFrame
used_df_with_cluster4 = pd.DataFrame(used_df_with_cluster4, columns=column_names)
# Add Cluster column to DataFrame
used_df_with_cluster4['Cluster'] = kmeans4_labels

# Group by Cluster column and calculate average
kmeans4_cluster_means = used_df_with_cluster4.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
print(kmeans4_cluster_means)

# 群中心的分散程度
inertia2 = kmeans2.inertia_
inertia3 = kmeans3.inertia_
inertia4 = kmeans4.inertia_
print(f"Inertia (群中心的分散程度): {inertia2, inertia3,inertia4}")

# import matplotlib.pyplot as plt
# import numpy as np
# import seaborn as sns; sns.set()
# from sklearn.cluster import KMeans
# X = np.array(used_df)
# X

# kmeans = KMeans(n_clusters=3)
# ##  define a kmean model
# kmeans.fit(X)
# ## To a Kmean model by using data X
# ## fit: chạy Kmeans, run Kmeans model
# y_kmeans = kmeans.predict(X)
# ## To access the Kmean model result
# ## Mỗi sample thuộc cluster nào sẽ bỏ vào y_kmeans
# y_kmeans
# ## cho y_kmeans hiện ra

# X_with_cluster = np.column_stack((X, y_kmeans))
# X_with_cluster

"""####SOM"""

!pip install somlib

used_df

used_df = used_df.drop(columns='Cluster', axis=1)

from somlib import som
import random
from scipy.cluster.hierarchy import dendrogram, linkage
temp= np.array(used_df)
temp

samples = []
for i in range(len(used_df)):
    sample = temp[i]
    sample = sample.reshape(1, 6)
    samples.append(sample)
samples

s = som.SOM(neurons=(1,3), dimentions=6, n_iter=500, learning_rate=0.2)
s.train(samples)
print("SOM Cluster centres:", s.weights_)
print("SOM labels:", s.labels_)
result_SOM = s.predict(samples)

## 畫資料散佈圖
plt.scatter(temp[:, 0], temp[:, 1], c= s.labels_, s=20, cmap='coolwarm')
SOM_centers = s.weights_
## 畫群中心散佈圖
plt.scatter(SOM_centers[:, 0], SOM_centers[:, 1], c='blue', s=100, alpha=0.9);
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 取得 SOM 的權重作為群中心
SOM_centers = s.weights_

# 畫 3D 散點圖
fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')

# 散點圖
scatter = ax.scatter(temp[:, 0], temp[:, 1], temp[:, 2], c=s.labels_, cmap='coolwarm', s=20)

# 標籤
ax.set_xlabel('Average Glucose Level')
ax.set_ylabel('HDL')
ax.set_zlabel('LDL')

# 加入 colorbar
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

# 加入 SOM 群中心
ax.scatter(SOM_centers[:, 0], SOM_centers[:, 1], SOM_centers[:,2], c='black', s=100, marker='X', label='SOM Centers')

# 設置視角
ax.view_init(elev=30, azim=-60)

# 顯示圖
plt.show()

from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

linked = linkage(SOM_centers, 'single')

color_threshold = 2.0
dendrogram(linked, orientation='top', labels=None, distance_sort='descending', show_leaf_counts=True, color_threshold=color_threshold)

plt.show()

from sklearn.metrics.pairwise import pairwise_distances_argmin_min
import numpy as np

distances_som = pairwise_distances_argmin_min(temp, SOM_centers)[1]
average_squared_distance_som = np.mean(np.square(distances_som))
print(f"Average Squared Distance in SOM: {average_squared_distance_som}")

from collections import Counter

# 使用 Counter 計算每個標籤的數量
SOM_sizes = Counter(s.labels_)

# 輸出每一群的數量
for label, size in SOM_sizes.items():
    print(f"Cluster {label + 1}: {size} samples")

from sklearn.metrics.pairwise import pairwise_distances
import numpy as np

# 假設你想要存儲每個中心點的平均距離
average_distances_to_centers = []

# 計算每個資料點到中心點的距離
distances_to_centers = pairwise_distances(temp, SOM_centers)

# 計算每個群內，中心與資料點的距離
for i, center in enumerate(SOM_centers):
    distances_to_current_center = distances_to_centers[:, i]
    average_distance_to_current_center = np.mean(distances_to_current_center)

    # 將每個中心點的平均距離加入到列表中
    average_distances_to_centers.append(average_distance_to_current_center)

    print(f"Average Distance to Center {i}: {average_distance_to_current_center}")

# 計算所有中心點的平均距離的平均值
total_average_distance = np.mean(average_distances_to_centers)

print(f'Total Average Distance to Centers: {total_average_distance}')

from sklearn.metrics.pairwise import euclidean_distances

# 計算每個資料點到其所屬群中心的距離
distances_to_centers = euclidean_distances(temp, SOM_centers)

# 計算群間平均距離
inter_cluster_distances = []
unique_labels = set(s.labels_)
for i, label_i in enumerate(unique_labels):
    for j, label_j in enumerate(unique_labels):
        if i < j:  # 避免計算兩次同樣的群組組合
            distances_between_clusters = euclidean_distances(SOM_centers[label_i].reshape(1, -1), SOM_centers[label_j].reshape(1, -1))
            inter_cluster_distances.extend(distances_between_clusters.flatten())

# 計算群和群間平均距離
average_inter_cluster_distance = np.mean(inter_cluster_distances)

print(f"Average Inter-Cluster Distance: {average_inter_cluster_distance}")



import pandas as pd

# Convert used_df to DataFrame
used_df = pd.DataFrame(used_df, columns=['Average Glucose Level', 'HDL','LDL', 'Stress Levels', 'Hypertension', 'Body Mass Index (BMI)'])

# Add Cluster column to DataFrame
used_df['Cluster'] = result_SOM

# Group by Cluster column and calculate average
SOM_cluster_means = used_df.groupby('Cluster').mean()

# Print out the average of each variable in each cluster
SOM_cluster_means

"""###Cluster summary

Average dispersion of each cluster

ward 4 group: 每個群的平均分散程度: 20320.4877218366

complete 6 group: 每個群的平均分散程度: 20108.352942287263

complete 5 group: 每個群的平均分散程度: 20108.352942287263

Inertia (群中心的分散程度2 group, 3 group): (10329895.694925599, 6957289.195621257)

seems like use complete to seperate to 5 or 6 group is better choice?


now see the complete 6 each cluster sizes:
Cluster 1: 2720 samples
Cluster 6: 2627 samples
Cluster 3: 2701 samples
Cluster 5: 2428 samples
Cluster 2: 2831 samples
Cluster 4: 1693 samples

now see the complete 5 each cluster sizes:
Cluster 2: 2720 samples
Cluster 1: 4320 samples
Cluster 3: 2701 samples
Cluster 5: 2428 samples
Cluster 4: 2831 samples

seems like 6 groups for each data sizes more balanced
try to decide to seperate it into 6 groups first
"""

used_df= used_df.drop('Cluster',axis=1)
used_df

used_df['Cluster'] = kmeans4_labels
kmeans_used_df = used_df
kmeans_used_df

# ward_used_df = used_df
# kmeans_used_df = used_df
# complete_used_df = used_df
# centroid_used_df = used_df


# 使用 pd.concat 合併兩個 DataFrame
split_used_df = pd.concat([kmeans_used_df, df_corr["Diagnosis"]], axis=1)

# 打印 split_used_df 的前幾行
print(split_used_df.head())

split_used_df

# 根據每個標籤分割資料
# df1 = split_used_df[cluster1_labels == 0]
# df2 = split_used_df[cluster1_labels == 1]
# df3 = split_used_df[cluster1_labels == 2]

df1 = split_used_df[split_used_df['Cluster'] == 0]
df2 = split_used_df[split_used_df['Cluster'] == 1]
df3 = split_used_df[split_used_df['Cluster'] == 2]
df4 = split_used_df[split_used_df['Cluster'] == 3]


# df1 = split_used_df[kmeans4_labels == 0]
# df2 = split_used_df[kmeans4_labels == 1]
# df3 = split_used_df[kmeans4_labels == 2]
# df4 = split_used_df[kmeans4_labels == 3]

# df1 = split_used_df[cluster3_labels == 0]
# df2 = split_used_df[cluster3_labels == 1]
# df3 = split_used_df[cluster3_labels == 2]

# cluster2_labels

# 輸出每個 DataFrame 的前幾行
print("\nDataFrame df1:")
print(df1.head())

print("\nDataFrame df2:")
print(df2.head())

print("\nDataFrame df3:")
print(df3.head())

print("\nDataFrame df4:")
print(df4.head())

def print_statistics(df, label):
    mean_diagnosis = np.mean(df['Diagnosis'])
    print(f"Mean diagnosis_{label}: {mean_diagnosis}")

    diagnosis_counts = df['Diagnosis'].value_counts()
    print(diagnosis_counts.to_string())  # Print without the Series information
    print('=====')

print_statistics(df1, "c1")
print_statistics(df2, "c2")
print_statistics(df3, "c3")
print_statistics(df4, "c4")

"""##SPLIT DATA INTO TRAINING AND TESTING DATA"""

from sklearn.model_selection import train_test_split
#1
X1 = df1.drop("Diagnosis", axis=1)
y1 = df1["Diagnosis"]
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# 2
X2 = df2.drop("Diagnosis", axis=1)
y2 = df2["Diagnosis"]
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)


# 3
X3 = df3.drop("Diagnosis", axis=1)
y3 = df3["Diagnosis"]
X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)

# 4
X4 = df4.drop("Diagnosis", axis=1)
y4 = df4["Diagnosis"]
X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=42)

print(X1_train.head())
print("===============")
print(y1_train.head())

"""##BUIDLING PREDICT MODEL

###DISCRIMINANT
"""

import numpy as np
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

"""####Cluster 1"""

temp_discriminant_1 = y1_train.values.flatten()
print(Counter(temp_discriminant_1))
### smote : balance training data
from imblearn.over_sampling import SMOTE
random_state = np.random.randint(0, 4294967295)
sm = SMOTE(random_state = random_state)
print(f"random_state used for this run: {random_state}")

X1_train_SM, y1_train_SM = sm.fit_resample(X1_train, y1_train)
print(Counter(y1_train_SM))
clf1 = LinearDiscriminantAnalysis()
clf1.fit(X1_train_SM, y1_train_SM)

#Result of y prediction
y1_predicted = clf1.predict(X1_test)

##Confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y1_test, y1_predicted)
print(confusion_matrix)

accuracy_discriminant_1 = accuracy_score(y1_test, y1_predicted)
print(f"Accuracy: {accuracy_discriminant_1}")

"""####Cluster 2"""

temp_discriminant_2 = y2_train.values.flatten()
print(Counter(temp_discriminant_2))
### smote : balance training data
from imblearn.over_sampling import SMOTE
random_state = np.random.randint(0, 4294967295)
sm = SMOTE(random_state = random_state)
print(f"random_state used for this run: {random_state}")

X2_train_SM, y2_train_SM = sm.fit_resample(X2_train, y2_train)
print(Counter(y2_train_SM))
clf2 = LinearDiscriminantAnalysis()
clf2.fit(X2_train_SM, y2_train_SM)

#Result of y prediction
y2_predicted = clf2.predict(X2_test)

##Confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y2_test, y2_predicted)
print(confusion_matrix)

accuracy_discriminant_2 = accuracy_score(y2_test, y2_predicted)
print(f"Accuracy: {accuracy_discriminant_2}")

"""####Cluster 3"""

temp_discriminant_3 = y3_train.values.flatten()
print(Counter(temp_discriminant_3))
### smote : balance training data
from imblearn.over_sampling import SMOTE
random_state = np.random.randint(0, 4294967295)
sm = SMOTE(random_state = random_state)
print(f"random_state used for this run: {random_state}")

X3_train_SM, y3_train_SM = sm.fit_resample(X3_train, y3_train)
print(Counter(y3_train_SM))
clf3 = LinearDiscriminantAnalysis()
clf3.fit(X3_train_SM, y3_train_SM)

#Result of y prediction
y3_predicted = clf3.predict(X3_test)

##Confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y3_test, y3_predicted)
print(confusion_matrix)

accuracy_discriminant_3 = accuracy_score(y3_test, y3_predicted)
print(f"Accuracy: {accuracy_discriminant_3}")

"""####Cluster 4"""

temp_discriminant_4 = y4_train.values.flatten()
print(Counter(temp_discriminant_4))
### smote : balance training data
from imblearn.over_sampling import SMOTE
random_state = np.random.randint(0, 4294967295)
sm = SMOTE(random_state = random_state)
print(f"random_state used for this run: {random_state}")

X4_train_SM, y4_train_SM = sm.fit_resample(X4_train, y4_train)
print(Counter(y4_train_SM))
clf4 = LinearDiscriminantAnalysis()
clf4.fit(X4_train_SM, y4_train_SM)

#Result of y prediction
y4_predicted = clf4.predict(X4_test)

##Confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y4_test, y4_predicted)
print(confusion_matrix)

accuracy_discriminant_4 = accuracy_score(y4_test, y4_predicted)
print(f"Accuracy: {accuracy_discriminant_4}")

"""###DecisionTree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler

"""####Cluster 1"""

temp_DecisionTree_1=y1_train.values.flatten()
print(Counter(temp_DecisionTree_1))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X1_train_SM, y1_train_SM = sm.fit_resample(X1_train, y1_train)
print(Counter(y1_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a CART template
clf1= DecisionTreeClassifier(max_depth =5, random_state = random_state)
# Train the model on the training data
clf1.fit(X1_train_SM, y1_train_SM)
y1_predicted = clf1.predict(X1_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y1_test, y1_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_DecisionTree_1 = accuracy_score(y1_test, y1_predicted)
print("Accuracy:", accuracy_DecisionTree_1)
print(f"random_state used for this run: {random_state}")

"""####Cluster 2"""

temp_DecisionTree_2=y2_train.values.flatten()
print(Counter(temp_DecisionTree_2))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X2_train_SM, y2_train_SM = sm.fit_resample(X2_train, y2_train)
print(Counter(y2_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a CART template
clf2= DecisionTreeClassifier(max_depth =5, random_state = random_state)
# Train the model on the training data
clf2.fit(X2_train_SM, y2_train_SM)
y2_predicted = clf2.predict(X2_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y2_test, y2_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_DecisionTree_2 = accuracy_score(y2_test, y2_predicted)
print("Accuracy:", accuracy_DecisionTree_2)
print(f"random_state used for this run: {random_state}")

"""####Cluster 3"""

temp_DecisionTree_3=y3_train.values.flatten()
print(Counter(temp_DecisionTree_3))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X3_train_SM, y3_train_SM = sm.fit_resample(X3_train, y3_train)
print(Counter(y3_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a CART template
clf3= DecisionTreeClassifier(max_depth =5, random_state = random_state)
# Train the model on the training data
clf3.fit(X3_train_SM, y3_train_SM)
y3_predicted = clf3.predict(X3_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y3_test, y3_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_DecisionTree_3 = accuracy_score(y3_test, y3_predicted)
print("Accuracy:", accuracy_DecisionTree_3)
print(f"random_state used for this run: {random_state}")

"""####Cluster 4"""

temp_DecisionTree_4=y4_train.values.flatten()
print(Counter(temp_DecisionTree_4))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X4_train_SM, y4_train_SM = sm.fit_resample(X4_train, y4_train)
print(Counter(y4_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a CART template
clf4= DecisionTreeClassifier(max_depth =5, random_state = random_state)
# Train the model on the training data
clf4.fit(X4_train_SM, y4_train_SM)
y4_predicted = clf4.predict(X4_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y4_test, y4_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_DecisionTree_4 = accuracy_score(y4_test, y4_predicted)
print("Accuracy:", accuracy_DecisionTree_4)
print(f"random_state used for this run: {random_state}")

"""###Neural Network"""

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler

"""####Cluster 1"""

temp_neural_network_1=y1_train.values.flatten()
print(Counter(temp_neural_network_1))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X1_train_SM, y1_train_SM = sm.fit_resample(X1_train, y1_train)
print(Counter(y1_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a MLP template
clf1= MLPClassifier(alpha=0.5, max_iter=1000, random_state=random_state)
# Train the model on the training data
clf1.fit(X1_train_SM, y1_train_SM)
y1_predicted = clf1.predict(X1_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y1_test, y1_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_neural_network_1 = accuracy_score(y1_test, y1_predicted)
print("Accuracy:", accuracy_neural_network_1)
print(f"random_state used for this run: {random_state}")

"""####Cluster 2"""

temp_neural_network_2=y2_train.values.flatten()
print(Counter(temp_neural_network_2))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X2_train_SM, y2_train_SM = sm.fit_resample(X2_train, y2_train)
print(Counter(y2_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a MLP template
clf2= MLPClassifier(alpha=0.5, max_iter=1000, random_state=random_state)
# Train the model on the training data
clf2.fit(X2_train_SM, y2_train_SM)
y2_predicted = clf2.predict(X2_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y2_test, y2_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_neural_network_2 = accuracy_score(y2_test, y2_predicted)
print("Accuracy:", accuracy_neural_network_2)
print(f"random_state used for this run: {random_state}")

"""####Cluster 3"""

temp_neural_network_3=y3_train.values.flatten()
print(Counter(temp_neural_network_2))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X3_train_SM, y3_train_SM = sm.fit_resample(X3_train, y3_train)
print(Counter(y2_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a MLP template
clf3= MLPClassifier(alpha=0.5, max_iter=1000, random_state=random_state)
# Train the model on the training data
clf3.fit(X3_train_SM, y3_train_SM)
y3_predicted = clf3.predict(X3_test)

##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y3_test, y3_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_neural_network_3 = accuracy_score(y3_test, y3_predicted)
print("Accuracy:", accuracy_neural_network_3)
print(f"random_state used for this run: {random_state}")

"""####Cluster 4"""

temp_neural_network_4=y4_train.values.flatten()
print(Counter(temp_neural_network_2))
from imblearn.over_sampling import SMOTE
random_state_smote = np.random.randint(0, 4294967295)
sm = SMOTE(random_state=random_state_smote)
print(f"random_state_smote used for this run: {random_state_smote}")

X4_train_SM, y4_train_SM = sm.fit_resample(X4_train, y4_train)
print(Counter(y4_train_SM))

random_state = np.random.randint(0, 4294967295)
# Create a MLP template
clf4= MLPClassifier(alpha=0.5, max_iter=1000, random_state=random_state)
# Train the model on the training data
clf4.fit(X4_train_SM, y4_train_SM)
y4_predicted = clf4.predict(X4_test)
##Confusion matrix
from sklearn.metrics import accuracy_score, confusion_matrix
confusion_matrix = confusion_matrix(y4_test, y4_predicted)
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_neural_network_4 = accuracy_score(y4_test, y4_predicted)
print("Accuracy:", accuracy_neural_network_4)
print(f"random_state used for this run: {random_state}")

"""###PREDICT SUMMARY

Draw a chart to compare the accuracy value of 3 models
"""

import numpy as np
import matplotlib.pyplot as plt

# accuracies_discriminant = [accuracy_discriminant_1, accuracy_discriminant_2, accuracy_discriminant_3]
# accuracies_decision_tree = [accuracy_DecisionTree_1, accuracy_DecisionTree_2, accuracy_DecisionTree_3]
# accuracies_neural_network = [accuracy_neural_network_1, accuracy_neural_network_2, accuracy_neural_network_3]

accuracies_discriminant = [accuracy_discriminant_1, accuracy_discriminant_2, accuracy_discriminant_3, accuracy_discriminant_4]
accuracies_decision_tree = [accuracy_DecisionTree_1, accuracy_DecisionTree_2, accuracy_DecisionTree_3, accuracy_DecisionTree_4]
accuracies_neural_network = [accuracy_neural_network_1, accuracy_neural_network_2, accuracy_neural_network_3, accuracy_neural_network_4]

# Names of the cluster
# cluster = ['Cluster 1', 'Cluster 2', 'Cluster 3']
cluster = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4']

# Plotting the comparison chart
bar_width = 0.3
index = np.arange(len(cluster))

plt.figure(figsize=(10, 6))

# Bar chart for Discriminant
plt.bar(index - bar_width, accuracies_discriminant, color='green', width=bar_width, label='Discriminant', alpha=0.7)
# Bar chart for Decision Tree
plt.bar(index, accuracies_decision_tree, color='blue', width=bar_width, label='Decision Tree', alpha=0.7)
# Bar chart for Neural Network
plt.bar(index + bar_width, accuracies_neural_network, color='orange', width=bar_width, label='Neural Network', alpha=0.7)

# Displaying the accuracy values on each bar
for i, model in enumerate(cluster):
    plt.text(index[i] - bar_width, accuracies_discriminant[i] + 0.01, f'{accuracies_discriminant[i]:.2f}', ha='center', va='bottom', color='black', fontweight='bold')
    plt.text(index[i], accuracies_decision_tree[i] + 0.01, f'{accuracies_decision_tree[i]:.2f}', ha='center', va='bottom', color='black', fontweight='bold')
    plt.text(index[i] + bar_width, accuracies_neural_network[i] + 0.01, f'{accuracies_neural_network[i]:.2f}', ha='center', va='bottom', color='black', fontweight='bold')

# Setting up chart parameters
plt.xlabel('Cluster')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies')
plt.xticks(index, cluster)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Displaying the chart
plt.show()

mean_discriminant = np.mean([accuracy_discriminant_1, accuracy_discriminant_2, accuracy_discriminant_3, accuracy_discriminant_4])
mean_decision_tree = np.mean([accuracy_DecisionTree_1, accuracy_DecisionTree_2, accuracy_DecisionTree_3, accuracy_DecisionTree_4])
mean_neural_network = np.mean([accuracy_neural_network_1, accuracy_neural_network_2, accuracy_neural_network_3, accuracy_neural_network_4])
print("mean accuracy discriminant:", mean_discriminant)
print("mean accuracy decision_tree:", mean_decision_tree)
print("mean accuracy neural_network:", mean_neural_network)

"""Other checks"""

df_corr

Diagnosis_features_means = df_corr.groupby('Diagnosis').mean()
print(Diagnosis_features_means)